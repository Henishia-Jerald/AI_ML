{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc831cf-a308-4541-86d7-16165d5df0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import playsound #pip install playsound==1.2.2\n",
    "import datetime\n",
    "import transformers\n",
    "import json\n",
    "from __future__ import print_function\n",
    "import random\n",
    "from transformers import logging as hf_logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a44a1212-3544-4b24-8d46-fe537fcf85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7024b03-456e-4dde-b44a-e7d893682503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\henis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\henis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\henis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\henis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\henis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7e4c9f-bcae-4e86-b025-3da73e2c07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To disable the \"Some layers from the model checkpoint at bert-base-cased were not used when initializing\"\n",
    "hf_logging.set_verbosity_error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b53f2c-fd61-43cc-996b-72dd7caf76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an AI\n",
    "class ChatBot():\n",
    "    @classmethod\n",
    "    def __init__(self,name):\n",
    "        print(\"--- Starting up\",name,\"---\")\n",
    "        self.name = name\n",
    "        \n",
    "    @classmethod   \n",
    "    def speech_to_text(self):\n",
    "            recognizer = sr.Recognizer()\n",
    "            with sr.Microphone() as source:\n",
    "                 recognizer.adjust_for_ambient_noise(source, duration=1)            \n",
    "                 print(\"Please say something...\")\n",
    "                 audio = recognizer.listen(source)\n",
    "            try:\n",
    "                 self.text = recognizer.recognize_google(audio)\n",
    "                 print(\"me --> \", self.text)\n",
    "            except:\n",
    "                 print(\"me -->  ERROR\")   \n",
    "                    \n",
    "    @classmethod\n",
    "    def wake_up(self,text):\n",
    "        self.text = text\n",
    "        return True if self.name.lower() in self.text.lower() else False\n",
    "   \n",
    "        os.system(\"start res.mp3\")\n",
    "        os.remove(\"res.mp3\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def text_to_speech(text):\n",
    "        print(\"ai --> \", text)\n",
    "        # speaker = gTTS(text=text, lang=\"en\", slow=False)\n",
    "        speaker = gTTS(text=text, lang=\"en\")\n",
    "        filename = \"res.mp3\"\n",
    "        speaker.save(filename)\n",
    "        playsound.playsound(filename)\n",
    "        os.remove(filename)\n",
    "        \n",
    "    @staticmethod\n",
    "    def action_time():\n",
    "        return datetime.datetime.now().time().strftime('%H:%M %p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac81711c-e68d-4af3-8b28-22009f3d4f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting up Joshua ---\n",
      "Please say something...\n",
      "me -->  hello Joshua\n",
      "ai -->  Hello I am Joshua the AI, what can I do for you?\n",
      "Please say something...\n",
      "me -->  what support is offered\n",
      "ai -->  Offering support for Adverse drug reaction, Blood pressure, Hospitals and Pharmacies\n",
      "Please say something...\n",
      "me -->  drug causing adverse\n",
      "ai -->  Navigating to Adverse drug reaction module\n",
      "Please say something...\n",
      "me -->  find me a pharmacy\n",
      "ai -->  Please provide pharmacy name\n",
      "Please say something...\n",
      "me -->  thank you for helping\n",
      "ai -->  Happy to help!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m ai \u001b[38;5;241m=\u001b[39m ChatBot(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJoshua\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeech_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Normalize casings for the user text which had been converted from user voice\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [ai\u001b[38;5;241m.\u001b[39mtext]\n",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m, in \u001b[0;36mChatBot.speech_to_text\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     10\u001b[0m recognizer \u001b[38;5;241m=\u001b[39m sr\u001b[38;5;241m.\u001b[39mRecognizer()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mMicrophone() \u001b[38;5;28;01mas\u001b[39;00m source:\n\u001b[1;32m---> 12\u001b[0m      \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_for_ambient_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m            \n\u001b[0;32m     13\u001b[0m      \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease say something...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m      audio \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mlisten(source)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\speech_recognition\\__init__.py:393\u001b[0m, in \u001b[0;36mRecognizer.adjust_for_ambient_noise\u001b[1;34m(self, source, duration)\u001b[0m\n\u001b[0;32m    391\u001b[0m elapsed_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m seconds_per_buffer\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m elapsed_time \u001b[38;5;241m>\u001b[39m duration: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m energy \u001b[38;5;241m=\u001b[39m audioop\u001b[38;5;241m.\u001b[39mrms(buffer, source\u001b[38;5;241m.\u001b[39mSAMPLE_WIDTH)  \u001b[38;5;66;03m# energy of the audio signal\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# dynamically adjust the energy threshold using asymmetric weighted average\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\speech_recognition\\__init__.py:199\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyaudio.py:608\u001b[0m, in \u001b[0;36mStream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    606\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ai = ChatBot(name=\"Joshua\")\n",
    "    \n",
    "    while True:\n",
    "        ai.speech_to_text()\n",
    "\n",
    "        # Normalize casings for the user text which had been converted from user voice\n",
    "        texts = [ai.text]\n",
    "        case_norm = [sentence.lower() for sentence in texts]\n",
    "        \n",
    "        # Tokenize the reviews using NLTKs word_tokenize function.\n",
    "        tokens = []\n",
    "        for item in case_norm:\n",
    "            tokens.append(word_tokenize(item)) \n",
    "            \n",
    "        # Perform parts-of-speech tagging on each sentence using the NLTK POS tagger.\n",
    "        token_tag = []\n",
    "        for token in tokens:\n",
    "            token_tag.append(pos_tag(token))\n",
    "            \n",
    "            \n",
    "        token_noun = []\n",
    "        for token in token_tag:\n",
    "            for tok,tag in token:\n",
    "                if tag.startswith('NN'):\n",
    "                    token_noun.append((tok,tag)) \n",
    "        \n",
    "        #Lemmatize.\n",
    "        #Different forms of the terms need to be treated as one.\n",
    "        #No need to provide POS tag to lemmatizer for now.\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token_lemmatized = [lemmatizer.lemmatize(tokens[0]) for tokens in token_noun]\n",
    "        \n",
    "        # Remove stopwords and punctuation (if there are any).\n",
    "        stop_words = stopwords.words('english')        \n",
    "        token_stop = []\n",
    "        token_stop.append([token for token in token_lemmatized if not token in stop_words])\n",
    "        \n",
    "        \n",
    "        puctuation_list = list(punctuation)\n",
    "        token_punct = []\n",
    "        for token in token_stop:\n",
    "            if not token in puctuation_list:\n",
    "                token_punct.append(token)\n",
    "        token_punct = np.array(token_punct).ravel()      \n",
    "        \n",
    "        # Opening the file which has the commands to be included into our chatBot \n",
    "        f = open('commands.json')\n",
    "        jsn = json.load(f)\n",
    "        jsn_array = jsn['intents'] \n",
    "        \n",
    "        # Creating the JSON array into dataframe\n",
    "        # separating the tag, patterns and responses into separate arrays and\n",
    "        # mapping the corresponding three items into a list\n",
    "        \n",
    "        df = pd.DataFrame(jsn_array)\n",
    "        pat = [pattern for pattern in df.patterns]\n",
    "        tag = [tag for tag in df.tag]\n",
    "        res = [response for response in df.responses]\n",
    "        tuples = list(map(lambda x,y,z: (x,y,z), tag, pat,res))\n",
    "        \n",
    "        # Choosing the correct tag,pattern and responses according to the user text which had been converted from the user voice\n",
    "        p = []\n",
    "        t = []\n",
    "        r = []\n",
    "        for tag,patterns,response in tuples:\n",
    "            for st in patterns:\n",
    "                if any(x in st for x in token_punct):\n",
    "                    r.append(response)\n",
    "                    t.append(tag)\n",
    "                    p.append(st)\n",
    "                    respond = np.array(r).ravel()\n",
    "                    \n",
    "            \n",
    "        # Calling the wake_up definition with the user voice or text and getting the name as response\n",
    "        temp = ai.wake_up(ai.text)\n",
    " \n",
    "        #If the name\n",
    "        if temp is True:\n",
    "            res = \"Hello I am Joshua the AI, what can I do for you?\"\n",
    "            \n",
    "        # If the user voice asks for \"time\"\n",
    "        elif \"time\" in ai.text:\n",
    "            res = ai.action_time()\n",
    "            \n",
    "       # If user asks for any of the classes from the file the chatbot responds with one of the responses from the JSON file\n",
    "        elif len(p)!=0: \n",
    "            res = np.random.choice(respond)            \n",
    "\n",
    "        else:\n",
    "            res = \"error in calling\"\n",
    "            \n",
    "        # Calling the definition which converts the AI text into speech\n",
    "        ai.text_to_speech(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9bc46-cf80-41f4-bd93-060cceb22977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
